{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ec13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, DataCollatorForSeq2Seq\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from metrics import compute_metrics\n",
    "from custom_trainer import WeightedLossT5\n",
    "from config import SEPARATOR\n",
    "\n",
    "# Lower precision for higher performance (negligible impact on results)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DATA_DOCUMENTS = \"/home/nub/Bachelor/bachelor-thesis/data/processed/documents.csv\"\n",
    "DATA_DOCUMENTS_AUG = \"/home/nub/Bachelor/bachelor-thesis/data/processed/documents_aug.csv\"\n",
    "DATA_TRAIN_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/train.csv\"\n",
    "DATA_EVAL_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/eval.csv\"\n",
    "DATA_TEST_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/test.csv\"\n",
    "MODELS_DIR = \"/home/nub/Bachelor/bachelor-thesis/models\"\n",
    "\n",
    "num_cpus = 16\n",
    "\n",
    "# Enable debug to drastically reduce values\n",
    "DEBUG = False\n",
    "DEBUG_SIZE = 4\n",
    "SPLITS = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "USE_COT = False\n",
    "USE_AUG = False\n",
    "USE_INDEXING = False\n",
    "\n",
    "SAVE_DIR = \"/home/nub/Bachelor/bachelor-thesis/models/finqa_full_base_pseudo\"\n",
    "\n",
    "\n",
    "peft_model_path = SAVE_DIR\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    cache_dir=MODELS_DIR,\n",
    "    local_files_only=True,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = WeightedLossT5.from_pretrained(\n",
    "    base_model_name,\n",
    "    cache_dir=MODELS_DIR,\n",
    "    local_files_only=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.tokenizer = tokenizer\n",
    "base_model.use_cot = USE_COT\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9723b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, target):\n",
    "    # Model will silently truncate above 512 tokens\n",
    "    model_inputs = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=target,\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def build_process_fn(input_key: str, indexing: bool, use_cot: bool):\n",
    "    if indexing:\n",
    "        if use_cot:\n",
    "            prefix = \"Retrieve the document id by reasoning step-by-step: Document: \"\n",
    "        else:\n",
    "            prefix = \"Retrieve the document id: Document: \"\n",
    "    else:\n",
    "        if use_cot:\n",
    "            prefix = \"Answer the question with a document id by reasoning step-by-step: Question: \"\n",
    "        else:\n",
    "            prefix = \"Answer the question with a document id: Question: \"\n",
    "\n",
    "    def process_examples(examples):\n",
    "        prompts = []\n",
    "        answers = []\n",
    "        for input_text, docid in zip(examples[input_key], examples[\"document_id\"]):\n",
    "            company, year, *keywords = docid.split(SEPARATOR)\n",
    "\n",
    "            prompt = prefix + f\"{company}-{year}, {input_text}\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "            if use_cot:\n",
    "                answer = (\n",
    "                    f\"This is about {company} in the year {year}. \"\n",
    "                    f\"Therefore, the final answer is {docid}.\"\n",
    "                )\n",
    "            else:\n",
    "                answer = docid\n",
    "            answers.append(answer)\n",
    "\n",
    "        tokenized = tokenize(prompts, answers)\n",
    "        return tokenized\n",
    "    return process_examples\n",
    "\n",
    "\n",
    "# Process documents for indexing\n",
    "if USE_AUG:\n",
    "    # Augmented documents use pseudo-queries which should use the retrieval task instead\n",
    "    input_key = \"pseudo_query\"\n",
    "    document_task = False\n",
    "    document_file = DATA_DOCUMENTS_AUG\n",
    "else:\n",
    "    # Traditional documents should use the indexing task\n",
    "    input_key = \"document\"\n",
    "    document_task = True\n",
    "    document_file = DATA_DOCUMENTS\n",
    "\n",
    "\n",
    "raw_documents_ds = load_dataset(\"csv\", data_files=document_file, split=\"train\")\n",
    "documents_ds = raw_documents_ds.map(\n",
    "    build_process_fn(input_key, document_task, USE_COT),\n",
    "    remove_columns=raw_documents_ds.column_names,\n",
    "    num_proc=num_cpus,\n",
    "    batched=True,\n",
    "    batch_size=max(1, len(raw_documents_ds) // num_cpus)\n",
    ")\n",
    "\n",
    "# Process data for retrieval (train, valid, test)\n",
    "file_mapping = {\n",
    "    \"train\": DATA_TRAIN_PROC,\n",
    "    \"eval\": DATA_EVAL_PROC,\n",
    "    \"test\": DATA_TEST_PROC,\n",
    "}\n",
    "\n",
    "# Process queries for retrieval\n",
    "raw_data_ds = load_dataset(\"csv\", data_files=file_mapping)\n",
    "tokenized_ds = raw_data_ds.map(\n",
    "    build_process_fn(\"question\", False, USE_COT),\n",
    "    remove_columns=raw_data_ds[\"train\"].column_names,\n",
    "    num_proc=num_cpus,\n",
    "    batched=True,\n",
    "    batch_size=max(1, len(raw_data_ds[\"eval\"]) // num_cpus)\n",
    ")\n",
    "\n",
    "# Merge the indexing stage into the train split\n",
    "if USE_INDEXING:\n",
    "    tokenized_ds[\"train\"] = concatenate_datasets([tokenized_ds[\"train\"], documents_ds])\n",
    "\n",
    "# Reduce data size for all splits\n",
    "if DEBUG:\n",
    "    for split in SPLITS:\n",
    "        tokenized_ds[split] = tokenized_ds[split].select(range(DEBUG_SIZE))\n",
    "        raw_data_ds[split] = raw_data_ds[split].select(range(DEBUG_SIZE))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3578dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [04:37<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:38<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:49<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'penalty_scaled': np.float64(1.0127699568069108), 'penalty_capped': np.float64(1.0127699568069108), 'penalty_uncapped': np.float64(1.0127699568069108), 'missing': 0.0, 'exact_match_accuracy': 0.9705647096464566, 'part_match_accuracy': 0.9849357436143541, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}, 'eval': {'penalty_scaled': np.float64(1.2865515288788223), 'penalty_capped': np.float64(1.2865515288788223), 'penalty_uncapped': np.float64(1.2865515288788223), 'missing': 0.0, 'exact_match_accuracy': 0.45300113250283125, 'part_match_accuracy': 0.6543978859947137, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}, 'test': {'penalty_scaled': np.float64(1.2886878814298168), 'penalty_capped': np.float64(1.2886878814298168), 'penalty_uncapped': np.float64(1.2886878814298168), 'missing': 0.0, 'exact_match_accuracy': 0.44638186573670446, 'part_match_accuracy': 0.6519907003778002, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0 if DEBUG else num_cpus,\n",
    "        prefetch_factor=None if DEBUG else 2,\n",
    "        pin_memory=False if DEBUG else True,\n",
    "    )\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=10,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        decoded_preds = model.base_model.decode_tokens(generated_ids.detach().cpu().numpy())\n",
    "        decoded_labels = model.base_model.decode_tokens(labels.detach().cpu().numpy())\n",
    "\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_labels.extend(decoded_labels)\n",
    "    \n",
    "    metrics, _ = compute_metrics(all_preds, all_labels, USE_COT)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "results = {}\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"Evaluating split: {split}\")\n",
    "    results[split] = {\n",
    "        **evaluate_model(model, tokenized_ds[split])\n",
    "    }\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deda336",
   "metadata": {},
   "source": [
    "{'train': {'penalty_scaled': np.float64(1.0127699568069108), 'penalty_capped': np.float64(1.0127699568069108), 'penalty_uncapped': np.float64(1.0127699568069108), 'missing': 0.0, 'exact_match_accuracy': 0.9705647096464566, 'part_match_accuracy': 0.9849357436143541, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "'eval': {'penalty_scaled': np.float64(1.2865515288788223), 'penalty_capped': np.float64(1.2865515288788223), 'penalty_uncapped': np.float64(1.2865515288788223), 'missing': 0.0, 'exact_match_accuracy': 0.45300113250283125, 'part_match_accuracy': 0.6543978859947137, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "'test': {'penalty_scaled': np.float64(1.2886878814298168), 'penalty_capped': np.float64(1.2886878814298168), 'penalty_uncapped': np.float64(1.2886878814298168), 'missing': 0.0, 'exact_match_accuracy': 0.44638186573670446, 'part_match_accuracy': 0.6519907003778002, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cadaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"abc\", \"wes\", \"aggs\", \"abc\"]\n",
    "b = [\"abc\", \"ws\", \"abc\", \"wes\"]\n",
    "len(set(a).intersection(set(b))) / len(set(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
