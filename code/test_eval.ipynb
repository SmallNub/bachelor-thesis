{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ec13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, DataCollatorForSeq2Seq\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from metrics import compute_metrics\n",
    "from process_data import build_process_fn\n",
    "from custom_trainer import WeightedLossT5\n",
    "from config import SEPARATOR\n",
    "\n",
    "# Lower precision for higher performance (negligible impact on results)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DATA_DOCUMENTS = \"/home/nub/Bachelor/bachelor-thesis/data/processed/documents.csv\"\n",
    "DATA_DOCUMENTS_AUG = \"/home/nub/Bachelor/bachelor-thesis/data/processed/documents_aug.csv\"\n",
    "DATA_TRAIN_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/train.csv\"\n",
    "DATA_EVAL_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/eval.csv\"\n",
    "DATA_TEST_PROC = \"/home/nub/Bachelor/bachelor-thesis/data/processed/test.csv\"\n",
    "MODELS_DIR = \"/home/nub/Bachelor/bachelor-thesis/models\"\n",
    "\n",
    "num_cpus = 16\n",
    "\n",
    "# Enable debug to drastically reduce values\n",
    "DEBUG = False\n",
    "DEBUG_SIZE = 4\n",
    "SPLITS = [\"train\", \"eval\", \"test\"]\n",
    "\n",
    "USE_COT = False\n",
    "USE_AUG = False\n",
    "USE_INDEXING = False\n",
    "\n",
    "SAVE_DIR = \"/home/nub/Bachelor/bachelor-thesis/models/finqa_full_base_low\"\n",
    "\n",
    "\n",
    "peft_model_path = SAVE_DIR\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    cache_dir=MODELS_DIR,\n",
    "    local_files_only=True,\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = WeightedLossT5.from_pretrained(\n",
    "    base_model_name,\n",
    "    cache_dir=MODELS_DIR,\n",
    "    local_files_only=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.tokenizer = tokenizer\n",
    "base_model.use_cot = USE_COT\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9723b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents for indexing\n",
    "if USE_AUG:\n",
    "    # Augmented documents use pseudo-queries which should use the retrieval task instead\n",
    "    input_key = \"pseudo_query\"\n",
    "    document_task = False\n",
    "    document_file = DATA_DOCUMENTS_AUG\n",
    "else:\n",
    "    # Traditional documents should use the indexing task\n",
    "    input_key = \"document\"\n",
    "    document_task = True\n",
    "    document_file = DATA_DOCUMENTS\n",
    "\n",
    "if USE_INDEXING:\n",
    "    raw_documents_ds = load_dataset(\"csv\", data_files=document_file, split=\"train\")\n",
    "    documents_ds = raw_documents_ds.map(\n",
    "        build_process_fn(tokenizer, input_key, document_task, USE_COT),\n",
    "        remove_columns=raw_documents_ds.column_names,\n",
    "        num_proc=num_cpus,\n",
    "        batched=True,\n",
    "        batch_size=max(1, len(raw_documents_ds) // num_cpus)\n",
    "    )\n",
    "\n",
    "# Process data for retrieval (train, valid, test)\n",
    "file_mapping = {\n",
    "    \"train\": DATA_TRAIN_PROC,\n",
    "    \"eval\": DATA_EVAL_PROC,\n",
    "    \"test\": DATA_TEST_PROC,\n",
    "}\n",
    "\n",
    "# Process queries for retrieval\n",
    "raw_data_ds = load_dataset(\"csv\", data_files=file_mapping)\n",
    "tokenized_ds = raw_data_ds.map(\n",
    "    build_process_fn(tokenizer, \"question\", False, USE_COT),\n",
    "    remove_columns=raw_data_ds[\"train\"].column_names,\n",
    "    num_proc=num_cpus,\n",
    "    batched=True,\n",
    "    batch_size=max(1, len(raw_data_ds[\"eval\"]) // num_cpus)\n",
    ")\n",
    "\n",
    "# Merge the indexing stage into the train split (overriden by sampling)\n",
    "if USE_INDEXING:\n",
    "    tokenized_ds[\"train\"] = concatenate_datasets([tokenized_ds[\"train\"], documents_ds])\n",
    "\n",
    "# Reduce data size for all splits\n",
    "if DEBUG:\n",
    "    raw_documents_ds = raw_documents_ds.select(range(DEBUG_SIZE))\n",
    "\n",
    "    for split in SPLITS:\n",
    "        tokenized_ds[split] = tokenized_ds[split].select(range(DEBUG_SIZE))\n",
    "        raw_data_ds[split] = raw_data_ds[split].select(range(DEBUG_SIZE))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3578dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [06:24<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:54<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [01:10<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'penalty_scaled': np.float64(1.0816701327787555), 'penalty_capped': np.float64(1.0816701327787555), 'penalty_uncapped': np.float64(1.0857974724044153), 'missing': 0.0, 'exact_match_accuracy': 0.9054551271796513, 'part_match_accuracy': 0.9445155441795946, 'set_match_accuracy': 0.9493840985442297, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}, 'eval': {'penalty_scaled': np.float64(1.4816459041147605), 'penalty_capped': np.float64(1.4816459041147605), 'penalty_uncapped': np.float64(1.5104643261608155), 'missing': 0.0, 'exact_match_accuracy': 0.4824462061155153, 'part_match_accuracy': 0.6753491883729695, 'set_match_accuracy': 0.6946772366930908, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}, 'test': {'penalty_scaled': np.float64(1.4557221737866899), 'penalty_capped': np.float64(1.4557221737866899), 'penalty_uncapped': np.float64(1.4821505376344086), 'missing': 0.0, 'exact_match_accuracy': 0.5047951176983435, 'part_match_accuracy': 0.6887532693984343, 'set_match_accuracy': 0.7145015983725687, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0 if DEBUG else num_cpus,\n",
    "        prefetch_factor=None if DEBUG else 2,\n",
    "        pin_memory=False if DEBUG else True,\n",
    "    )\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                max_new_tokens=128,\n",
    "                num_beams=10,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        \n",
    "        decoded_preds = model.base_model.decode_tokens(generated_ids.detach().cpu().numpy())\n",
    "        decoded_labels = model.base_model.decode_tokens(labels.detach().cpu().numpy())\n",
    "\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_labels.extend(decoded_labels)\n",
    "    \n",
    "    metrics, _ = compute_metrics(all_preds, all_labels, USE_COT)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "results = {}\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"Evaluating split: {split}\")\n",
    "    results[split] = {\n",
    "        **evaluate_model(model, tokenized_ds[split])\n",
    "    }\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deda336",
   "metadata": {},
   "source": [
    "/home/nub/Bachelor/bachelor-thesis/models/finqa_full_base_pseudo\n",
    "\n",
    "{'train': {'penalty_scaled': np.float64(1.0127699568069108), 'penalty_capped': np.float64(1.0127699568069108), 'penalty_uncapped': np.float64(1.0127699568069108), 'missing': 0.0, 'exact_match_accuracy': 0.9705647096464566, 'part_match_accuracy': 0.9849357436143541, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'eval': {'penalty_scaled': np.float64(1.2865515288788223), 'penalty_capped': np.float64(1.2865515288788223), 'penalty_uncapped': np.float64(1.2865515288788223), 'missing': 0.0, 'exact_match_accuracy': 0.45300113250283125, 'part_match_accuracy': 0.6543978859947137, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'test': {'penalty_scaled': np.float64(1.2886878814298168), 'penalty_capped': np.float64(1.2886878814298168), 'penalty_uncapped': np.float64(1.2886878814298168), 'missing': 0.0, 'exact_match_accuracy': 0.44638186573670446, 'part_match_accuracy': 0.6519907003778002, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642048b6",
   "metadata": {},
   "source": [
    "/home/nub/Bachelor/bachelor-thesis/models/finqa_full_base_prompt\n",
    "\n",
    "{'train': {'penalty_scaled': np.float64(1.1170916653335468), 'penalty_capped': np.float64(1.1170916653335468), 'penalty_uncapped': np.float64(1.1170916653335468), 'missing': 0.0, 'exact_match_accuracy': 0.8341065429531275, 'part_match_accuracy': 0.8994827494267531, 'set_match_accuracy': 0.908041380045855, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'eval': {'penalty_scaled': np.float64(1.3450887127217819), 'penalty_capped': np.float64(1.3450887127217819), 'penalty_uncapped': np.float64(1.3450887127217819), 'missing': 0.0, 'exact_match_accuracy': 0.5209513023782559, 'part_match_accuracy': 0.7095130237825605, 'set_match_accuracy': 0.7258588146470372, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'test': {'penalty_scaled': np.float64(1.3277361232199942), 'penalty_capped': np.float64(1.3277361232199942), 'penalty_uncapped': np.float64(1.3277361232199942), 'missing': 0.0, 'exact_match_accuracy': 0.5527462946817786, 'part_match_accuracy': 0.7213019471084022, 'set_match_accuracy': 0.740860215053766, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad6efe",
   "metadata": {},
   "source": [
    "/home/nub/Bachelor/bachelor-thesis/models/finqa_full_base_low\n",
    "\n",
    "{'train': {'penalty_scaled': np.float64(1.0816701327787555), 'penalty_capped': np.float64(1.0816701327787555), 'penalty_uncapped': np.float64(1.0857974724044153), 'missing': 0.0, 'exact_match_accuracy': 0.9054551271796513, 'part_match_accuracy': 0.9445155441795946, 'set_match_accuracy': 0.9493840985442297, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'eval': {'penalty_scaled': np.float64(1.4816459041147605), 'penalty_capped': np.float64(1.4816459041147605), 'penalty_uncapped': np.float64(1.5104643261608155), 'missing': 0.0, 'exact_match_accuracy': 0.4824462061155153, 'part_match_accuracy': 0.6753491883729695, 'set_match_accuracy': 0.6946772366930908, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0},\n",
    "\n",
    "'test': {'penalty_scaled': np.float64(1.4557221737866899), 'penalty_capped': np.float64(1.4557221737866899), 'penalty_uncapped': np.float64(1.4821505376344086), 'missing': 0.0, 'exact_match_accuracy': 0.5047951176983435, 'part_match_accuracy': 0.6887532693984343, 'set_match_accuracy': 0.7145015983725687, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
