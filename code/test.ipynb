{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50678542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca9e7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-10 21:50:50,963] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer  = T5Tokenizer.from_pretrained(model_name, cache_dir=\"/home/nub/Bachelor/bachelor-thesis/models\")\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=\"/home/nub/Bachelor/bachelor-thesis/models\").to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_RAW = \"data/raw\"\n",
    "DATA_DIR_PROC = \"data/processed\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 4096\n",
    "MAX_TARGET_LENGTH = 64\n",
    "USED_COLUMNS = [\"full_text\", \"table\", \"id\", \"question\", \"answer\", \"exe_ans\", \"steps\", \"program\", \"program_re\"]\n",
    "\n",
    "\n",
    "def convert_table(table: list[list[str]]):\n",
    "    \"\"\"Convert nested table structure to csv.\"\"\"\n",
    "    header, *rows = table\n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "\n",
    "def reformat_data(file_name: str):\n",
    "    \"\"\"Reformat the FinQA dataset.\"\"\"\n",
    "    raw_df = pd.read_json(os.path.join(DATA_DIR_RAW, file_name))\n",
    "    \n",
    "    # Unnest the question data\n",
    "    qa_df = pd.DataFrame(raw_df[\"qa\"].to_dict()).T\n",
    "    raw_df = pd.concat([raw_df, qa_df], axis=\"columns\")\n",
    "    \n",
    "    \n",
    "    raw_df.loc[:, \"pre_text\"] = raw_df[\"pre_text\"].map(\" \".join)\n",
    "    raw_df.loc[:, \"post_text\"] = raw_df[\"post_text\"].map(\" \".join)\n",
    "    raw_df.loc[:, \"table\"] = raw_df[\"table\"].map(convert_table)\n",
    "\n",
    "    raw_df.loc[:, \"full_text\"] = raw_df[\"pre_text\"] + raw_df[\"post_text\"] + \"\\nThis is a table:\\n\" + raw_df[\"table\"]\n",
    "    \n",
    "    # Drop the unused columns\n",
    "    df = raw_df[USED_COLUMNS]\n",
    "    df.to_csv(os.path.join(DATA_DIR_PROC, file_name))\n",
    "\n",
    "\n",
    "def create_documents_data(train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    document_columns = [\"full_text\", \"id\"]\n",
    "    documents_df = pd.concat([train_df[document_columns], valid_df[document_columns], test_df[document_columns]], axis=\"index\")\n",
    "    return documents_df\n",
    "\n",
    "\n",
    "def prepare_sample(data: pd.Series, tokenizer: T5Tokenizer):\n",
    "    prompt = f\"Generate the document ID for this text:\\n{data['full_text']}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        data[\"id\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = reformat_data(\"/home/nub/Bachelor/bachelor-thesis/FinQA-main/dataset/train.json\")\n",
    "valid_df = reformat_data(\"/home/nub/Bachelor/bachelor-thesis/FinQA-main/dataset/dev.json\")\n",
    "test_df = reformat_data(\"/home/nub/Bachelor/bachelor-thesis/FinQA-main/dataset/test.json\")\n",
    "documents_df = create_documents_data(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0329bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6251.000000\n",
       "mean      894.160934\n",
       "std       292.695415\n",
       "min       151.000000\n",
       "25%       718.000000\n",
       "50%       870.000000\n",
       "75%      1050.000000\n",
       "max      3713.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for i in data.index:\n",
    "    pre_text = len(data[\"pre_text\"][i].split())\n",
    "    post_text = len(data[\"post_text\"][i].split())\n",
    "    table = len(\" \".join([item for sublist in data[\"table\"][i] for item in sublist]).split())\n",
    "    lengths.append(pre_text + post_text + table)\n",
    "\n",
    "pd.Series(lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6822b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million . foreign currency exposure as more fully described in note 2i . in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s . dollar-based exposures by entering into forward foreign currency exchange contracts . the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months . currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses . relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates . the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings . we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties . the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s . dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .\n",
      ",october 31 2009,november 1 2008\n",
      "fair value of forward exchange contracts asset ( liability ),$ 6427,$ -23158 ( 23158 )\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ),$ 20132,$ -9457 ( 9457 )\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability,$ -6781 ( 6781 ),$ -38294 ( 38294 )\n",
      "\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) . . . . . . . . . $ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability . . . . . . . . . . . . . . . . . . . . . . $ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s . dollar . in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive . our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .\n"
     ]
    }
   ],
   "source": [
    "print(data[\"full_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e85576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    inputs  = tokenizer(example[\"text\"], truncation=True, max_length=8192, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"summary\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1afe82b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million . foreign currency exposure as more fully described in note 2i . in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s . dollar-based exposures by entering into forward foreign currency exchange contracts . the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months . currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses . relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates . the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings . we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties . the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s . dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .\n"
     ]
    }
   ],
   "source": [
    "print(data[\"pre_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91df2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADI/2009/page_49.pdf-1\n"
     ]
    }
   ],
   "source": [
    "table = \"year | 2020 | 2021 \\n movies | 12 | 23 \\n games | 67 | 54\"\n",
    "\n",
    "# document = data[\"table\"][0]\n",
    "# input_text = f\"You are a highly intelligent bot. How many games were there in 2020? Here is the table: \\n {table}\"\n",
    "# input_text = f\"You are a highly intelligent bot. The following text is made with HTML. What is second sentence:\\n <h1>I love walking on the beach.</h1><p>My dog is fat.</p><h2>He was yelling at a tree.</h2>\"\n",
    "# input_text = f\"Let's think step by step. Generate the document ID for the document:\\nI think we should go to the beach. Afterwards, we can go the cinema to watch a movie.\"\n",
    "input_text = f\"{train_df['id'][0]}\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d3eb91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADI/2009/page_49.pdf-1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=8192\n",
    ")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids.to(model.device),\n",
    "    attention_mask=inputs.attention_mask.to(model.device),\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "776e301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_glen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
