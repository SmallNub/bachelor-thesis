{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50678542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import multiprocessing\n",
    "from config import *\n",
    "\n",
    "num_cpus = int(os.getenv(\"SLURM_CPUS_PER_TASK\", multiprocessing.cpu_count()))\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-xl\"\n",
    "OUTPUT_DIR = os.path.join(MODELS_DIR, \"finqa_indexer\")\n",
    "DS_CONFIG = \"./ds_config.json\"\n",
    "\n",
    "train_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=steven.dong@student.uva.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca9e7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer  = T5Tokenizer.from_pretrained(model_name, cache_dir=\"/home/nub/Bachelor/bachelor-thesis/models\", use_fast=True)\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=\"/home/nub/Bachelor/bachelor-thesis/models\", device_map=\"auto\", local_files_only=True,\n",
    "    low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a654b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    "    bf16=True,\n",
    "    deepspeed=DS_CONFIG,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    predict_with_generate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445df5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\"csv\", data_files=\"/home/nub/Bachelor/bachelor-thesis/data/processed/documents.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e738bb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2ab9188abb4f01a6a50c4439aa1e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/8281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_fn(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"full_text\"],\n",
    "        truncation=True,\n",
    "        max_length=4096,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"id\"],\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": inputs.input_ids.squeeze(0),\n",
    "        \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "        \"labels\": targets.input_ids.squeeze(0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Map & set format for PyTorch\n",
    "tokenized_ds = raw_ds.map(\n",
    "    preprocess_fn,\n",
    "    remove_columns=raw_ds.column_names,\n",
    "    num_proc=num_cpus\n",
    ")\n",
    "tokenized_ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128a9fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  27,  183, 3214,   12,    8, 1078,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer(\n",
    "        \"I am walking to the store\",\n",
    "        truncation=True,\n",
    "        max_length=16,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "840a5143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    3, 27775,    87, 16660,    87,  6492,   834,  3647,     5, 17388,\n",
       "         2292,     1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[0][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb84d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8281.000000\n",
       "mean       11.812342\n",
       "std         0.672473\n",
       "min         9.000000\n",
       "25%        11.000000\n",
       "50%        12.000000\n",
       "75%        12.000000\n",
       "max        14.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = []\n",
    "for i in range(len(tokenized_ds)):\n",
    "    total.append(0 + torch.count_nonzero(tokenized_ds[i][\"labels\"]).numpy())\n",
    "pd.Series(total).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8239dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_RAW = \"data/raw\"\n",
    "DATA_DIR_PROC = \"data/processed\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 4096\n",
    "MAX_TARGET_LENGTH = 64\n",
    "USED_COLUMNS = [\"full_text\", \"table\", \"id\", \"question\", \"answer\", \"exe_ans\", \"steps\", \"program\", \"program_re\"]\n",
    "\n",
    "\n",
    "def convert_table(table: list[list[str]]):\n",
    "    \"\"\"Convert nested table structure to csv.\"\"\"\n",
    "    header, *rows = table\n",
    "    df = pd.DataFrame(rows, columns=header)\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "\n",
    "def reformat_data(file_name: str):\n",
    "    \"\"\"Reformat the FinQA dataset.\"\"\"\n",
    "    raw_df = pd.read_json(os.path.join(DATA_DIR_RAW, file_name))\n",
    "    \n",
    "    # Unnest the question data\n",
    "    qa_df = pd.DataFrame(raw_df[\"qa\"].to_dict()).T\n",
    "    raw_df = pd.concat([raw_df, qa_df], axis=\"columns\")\n",
    "    \n",
    "    \n",
    "    raw_df.loc[:, \"pre_text\"] = raw_df[\"pre_text\"].map(\" \".join)\n",
    "    raw_df.loc[:, \"post_text\"] = raw_df[\"post_text\"].map(\" \".join)\n",
    "    raw_df.loc[:, \"table\"] = raw_df[\"table\"].map(convert_table)\n",
    "\n",
    "    raw_df.loc[:, \"full_text\"] = raw_df[\"pre_text\"] + raw_df[\"post_text\"] + \"\\nThis is a table:\\n\" + raw_df[\"table\"]\n",
    "    \n",
    "    # Drop the unused columns\n",
    "    # df = raw_df[USED_COLUMNS]\n",
    "    # df.to_csv(os.path.join(DATA_DIR_PROC, file_name))\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def create_documents_data(train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    document_columns = [\"full_text\", \"id\"]\n",
    "    documents_df = pd.concat([train_df[document_columns], valid_df[document_columns], test_df[document_columns]], axis=\"index\")\n",
    "    return documents_df\n",
    "\n",
    "\n",
    "def prepare_sample(data: pd.Series, tokenizer: T5Tokenizer):\n",
    "    prompt = f\"Generate the document ID for this text:\\n{data['full_text']}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        data[\"id\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = reformat_data(\"/home/nub/Bachelor/bachelor-thesis/data/raw/train.json\")\n",
    "valid_df = reformat_data(\"/home/nub/Bachelor/bachelor-thesis/data/raw/dev.json\")\n",
    "test_df = reformat_data(\"//home/nub/Bachelor/bachelor-thesis/data/raw/test.json\")\n",
    "documents_df = create_documents_data(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0ae9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/nub/Bachelor/bachelor-thesis/data/processed/train.csv\")\n",
    "valid_df = pd.read_csv(\"/home/nub/Bachelor/bachelor-thesis/data/processed/valid.csv\")\n",
    "test_df = pd.read_csv(\"/home/nub/Bachelor/bachelor-thesis/data/processed/test.csv\")\n",
    "documents_df = pd.read_csv(\"/home/nub/Bachelor/bachelor-thesis/data/processed/documents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0329bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8281.000000\n",
       "mean      673.093225\n",
       "std       255.771629\n",
       "min        24.000000\n",
       "25%       535.000000\n",
       "50%       667.000000\n",
       "75%       811.000000\n",
       "max      2674.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "df = documents_df\n",
    "\n",
    "for i in df.index:\n",
    "    lengths.append(len(df[\"full_text\"][i].split()))\n",
    "\n",
    "pd.Series(lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6822b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million . foreign currency exposure as more fully described in note 2i . in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s . dollar-based exposures by entering into forward foreign currency exchange contracts . the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months . currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses . relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates . the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings . we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties . the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s . dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .\n",
      ",october 31 2009,november 1 2008\n",
      "fair value of forward exchange contracts asset ( liability ),$ 6427,$ -23158 ( 23158 )\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ),$ 20132,$ -9457 ( 9457 )\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability,$ -6781 ( 6781 ),$ -38294 ( 38294 )\n",
      "\n",
      "fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) . . . . . . . . . $ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability . . . . . . . . . . . . . . . . . . . . . . $ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s . dollar . in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive . our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .\n"
     ]
    }
   ],
   "source": [
    "print(data[\"full_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e85576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    inputs  = tokenizer(example[\"text\"], truncation=True, max_length=8192, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"summary\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1afe82b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million . foreign currency exposure as more fully described in note 2i . in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s . dollar-based exposures by entering into forward foreign currency exchange contracts . the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months . currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses . relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates . the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged . the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings . we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties . while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk . the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties . the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s . dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .\n"
     ]
    }
   ],
   "source": [
    "print(data[\"pre_text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91df2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADI/2009/page_49.pdf-1\n"
     ]
    }
   ],
   "source": [
    "table = \"year | 2020 | 2021 \\n movies | 12 | 23 \\n games | 67 | 54\"\n",
    "\n",
    "# document = data[\"table\"][0]\n",
    "# input_text = f\"You are a highly intelligent bot. How many games were there in 2020? Here is the table: \\n {table}\"\n",
    "# input_text = f\"You are a highly intelligent bot. The following text is made with HTML. What is second sentence:\\n <h1>I love walking on the beach.</h1><p>My dog is fat.</p><h2>He was yelling at a tree.</h2>\"\n",
    "# input_text = f\"Let's think step by step. Generate the document ID for the document:\\nI think we should go to the beach. Afterwards, we can go the cinema to watch a movie.\"\n",
    "input_text = f\"{train_df['id'][0]}\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3eb91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADI/2009/page_49.pdf-1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=8192\n",
    ")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    inputs.input_ids.to(model.device),\n",
    "    attention_mask=inputs.attention_mask.to(model.device),\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776e301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_glen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
