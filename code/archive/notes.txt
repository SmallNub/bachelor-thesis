training_12061830 - finqa_full_index - Incorrect training setup, only indexing
training_12076936 - finqa_full_base_bad - Correct training setup, no context
training_12081975 - finqa_full_cot_bad - Correct training setup, no context, zero score
training_12090958 - finqa_full_base_context - Context (company, year in query + docs)
training_12193923 - finqa_full_base_fail - Model failed to learn - Incorrect penalty
training_12194582 - finqa_full_base_fail2 - Model failed to learn (lower lr + penalties) - Incorrect penalty
training_12215509 - finqa_full_base_pseudo - Only pseudo-queries for indexing
training_12233893 - finqa_full_base_error - slight mistake in penalty, 5 pseudo, higher penalty, 0.2 dropout 0.05 label, CRASHED
training_12240069 - finqa_full_base_prompt - 5 pseudo, higher penalty, 0.3 dropout, 0.1 label, prompt sampling
training_12252717 - finqa_full_base_low - 5 pseudo, mid penalty, 0.2 dropout, 0.05 label, prompt sampling, 1e-4 lr, 0.7 - 3 reduce lr
training_12263893 - finqa_full_base_5 - 5 pseudo, mid log penalty, 0.2 dropout, 0.05 label, prompt sampling, 2e-4 lr, 0.5 - 1 reduce lr
training_12266640 - finqa_full_base_10 - 10 pseudo, mid log penalty, 0.2 dropout, 0.05 label, prompt sampling, 2e-4 lr, 0.5 - 1 reduce lr, CRASHED
training_12279861 - finqa_full_base_10_fail - incorrect resumption, RESUME
training_12281174 - finqa_full_base - no deepspeed, RETRY
training_12296260 - NONE - early crash
training_12297005 - finqa_base_10 - 10 pseudo, mid log penalty, 0.2 dropout, 0.05 label, prompt sampling, 2e-5 lr, 0.5 - 1 reduce lr, BASELINE
training_12368701 - finqa_base_10_cot - 10 pseudo, mid log penalty, 0.2 dropout, 0.05 label, prompt sampling, 2e-5 lr, 0.5 - 1 reduce lr, COT
training_12375528 - finqa_base_10_ex - 10 pseudo, mid log penalty, 0.2 dropout, 0.05 label, prompt sampling, 2e-5 lr, 0.5 - 1 reduce lr, EXAMPLES
training_12559361 - finqa_base_10_no_ex - SAME, NO CHECKPOINT FROM BASELINE, EXAMPLES
training_12559387 - finqa_base_10_no_cot - SAME, NO CHECKPOINT FROM BASELINE, COT
training_12632629 - finqa_base_10_full - SAME, BASELINE, NO LORA
training_12632676 - finqa_base_10_2 - SAME, BASELINE
training_12632683 - finqa_base_10_full_ex - SAME, BASELINE, NO LORA, EXAMPLES
training_12632694 - finqa_base_10_full_cot - SAME, BASELINE, NO LORA, COT

training_17848603 - reasongr_zero - improved regex
training_17848643 - reasongr_few - improved regex
training_17848730 - reasongr_cot - improved regex

NO CONSTRAINTS
eval_17854653 - reasongr_zero
{'train': {'penalty_scaled': np.float64(1.1802888704978267), 'penalty_capped': np.float64(1.1802888704978267), 'penalty_uncapped': np.float64(1.3024156135018399), 'missing': 0.0, 'exact_match_accuracy': 0.8073908174692049, 'part_match_accuracy': 0.8811123553564705, 'set_match_accuracy': 0.8899802698234897, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.8073908174692049, 'hits@10': 0.9323308270676691, 'mrr': 0.8605265697170992, 'ndcg@10': 0.8786974863040529}, 'eval': {'penalty_scaled': np.float64(1.411496454539219), 'penalty_capped': np.float64(1.411496454539219), 'penalty_uncapped': np.float64(1.6938278595696492), 'missing': 0.0, 'exact_match_accuracy': 0.565118912797282, 'part_match_accuracy': 0.728010570026426, 'set_match_accuracy': 0.7437901094752745, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.565118912797282, 'hits@10': 0.82559456398641, 'mrr': 0.6592289992629743, 'ndcg@10': 0.7002305076496766}, 'test': {'penalty_scaled': np.float64(1.3911865739593878), 'penalty_capped': np.float64(1.3911865739593878), 'penalty_uncapped': np.float64(1.6596338273757631), 'missing': 0.0, 'exact_match_accuracy': 0.5884917175239756, 'part_match_accuracy': 0.7409183376925348, 'set_match_accuracy': 0.7582098227259543, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.5884917175239756, 'hits@10': 0.8369659982563208, 'mrr': 0.6837958926668604, 'ndcg@10': 0.7218639067160437}}

eval_17854664 - reasongr_few
{'train': {'penalty_scaled': np.float64(1.1121922492795056), 'penalty_capped': np.float64(1.1121922492795056), 'penalty_uncapped': np.float64(1.1878979363301871), 'missing': 0.0, 'exact_match_accuracy': 0.8796992481203008, 'part_match_accuracy': 0.9260385005065829, 'set_match_accuracy': 0.9320802005012514, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.8796992481203008, 'hits@10': 0.9643257078867381, 'mrr': 0.9162724440280031, 'ndcg@10': 0.9284842736497779}, 'eval': {'penalty_scaled': np.float64(1.3665347394426994), 'penalty_capped': np.float64(1.3665347394426994), 'penalty_uncapped': np.float64(1.620064175160438), 'missing': 0.0, 'exact_match_accuracy': 0.6172140430351076, 'part_match_accuracy': 0.7567006417516059, 'set_match_accuracy': 0.7713476783691966, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.6172140430351076, 'hits@10': 0.8471121177802945, 'mrr': 0.7015077567455823, 'ndcg@10': 0.7371783139584827}, 'test': {'penalty_scaled': np.float64(1.3590995338476692), 'penalty_capped': np.float64(1.3590995338476692), 'penalty_uncapped': np.float64(1.606190061028771), 'missing': 0.0, 'exact_match_accuracy': 0.6242371403661726, 'part_match_accuracy': 0.7612612612612648, 'set_match_accuracy': 0.7801511188607991, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.6242371403661726, 'hits@10': 0.8456843940714909, 'mrr': 0.7093940714908457, 'ndcg@10': 0.7432476736756797}}

eval_17854673 - reasongr_cot
{'train': {'penalty_scaled': np.float64(1.1327400097431874), 'penalty_capped': np.float64(1.1327400097431874), 'penalty_uncapped': np.float64(1.2214818962299365), 'missing': 0.0, 'exact_match_accuracy': 0.8568229083346665, 'part_match_accuracy': 0.9127606249666682, 'set_match_accuracy': 0.9203114168399692, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.8568229083346665, 'hits@10': 0.9542473204287314, 'mrr': 0.899403968380932, 'ndcg@10': 0.9133633139698939}, 'eval': {'penalty_scaled': np.float64(1.4220405889271277), 'penalty_capped': np.float64(1.4220405889271277), 'penalty_uncapped': np.float64(1.7122876557191395), 'missing': 0.0, 'exact_match_accuracy': 0.5571913929784824, 'part_match_accuracy': 0.7202718006795021, 'set_match_accuracy': 0.7389580973952438, 'structure_score_norm': -0.0011325028312570782, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0007550018875047187, 'hits@1': 0.5571913929784824, 'hits@10': 0.796149490373726, 'mrr': 0.647881950062018, 'ndcg@10': 0.6846343206695917}, 'test': {'penalty_scaled': np.float64(1.3890450159410774), 'penalty_capped': np.float64(1.3890450159410774), 'penalty_uncapped': np.float64(1.653821563498983), 'missing': 0.0, 'exact_match_accuracy': 0.5858761987794245, 'part_match_accuracy': 0.7428073234524879, 'set_match_accuracy': 0.7625399593141557, 'structure_score_norm': 0.0, 'structure_score_pos': 0.0, 'structure_score_neg': 0.0, 'hits@1': 0.5858761987794245, 'hits@10': 0.8230165649520488, 'mrr': 0.6748830627862886, 'ndcg@10': 0.7115615836595596}}

CONSTRAINTS
eval_17854653 - reasongr_zero

eval_17854664 - reasongr_few

eval_17855908 - reasongr_cot
